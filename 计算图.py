import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

print("=" * 70)
print("loss.backward() å¦‚ä½•æ‰¾åˆ°è®¡ç®—å›¾çš„æ·±å±‚æœºåˆ¶è§£æ")
print("=" * 70)

# ============================
# 1. è®¡ç®—å›¾å­˜å‚¨åœ¨tensoræœ¬èº«
# ============================
print("\n1. è®¡ç®—å›¾å­˜å‚¨åœ¨tensoræœ¬èº«")
print("-" * 50)

tensor_graph_explanation = """
ğŸ”— æ ¸å¿ƒæœºåˆ¶ï¼šè®¡ç®—å›¾å­˜å‚¨åœ¨tensorçš„å±æ€§ä¸­

ğŸ“Š **æ¯ä¸ªtensoréƒ½æºå¸¦è®¡ç®—å›¾ä¿¡æ¯**ï¼š
â€¢ grad_fn: æŒ‡å‘åˆ›å»ºè¯¥tensorçš„å‡½æ•°èŠ‚ç‚¹
â€¢ requires_grad: æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦
â€¢ is_leaf: æ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹ï¼ˆç”¨æˆ·åˆ›å»ºçš„tensorï¼‰

ğŸ¯ **loss.backward() çš„å·¥ä½œåŸç†**ï¼š
â€¢ loss tensoræœ¬èº«å°±"çŸ¥é“"æ•´ä¸ªè®¡ç®—å›¾
â€¢ é€šè¿‡grad_fnå±æ€§ï¼Œå¯ä»¥è®¿é—®æ•´ä¸ªè®¡ç®—é“¾
â€¢ ä»losså¼€å§‹ï¼Œé€’å½’éå†æ‰€æœ‰çˆ¶èŠ‚ç‚¹

ğŸ’¡ **å…³é”®ç†è§£**ï¼š
è®¡ç®—å›¾ä¸æ˜¯ç‹¬ç«‹å­˜åœ¨çš„æ•°æ®ç»“æ„ï¼Œè€Œæ˜¯åˆ†å¸ƒå¼å­˜å‚¨åœ¨å„ä¸ªtensorä¸­ï¼
"""
print(tensor_graph_explanation)

# ============================
# 2. æ¼”ç¤ºtensorå¦‚ä½•æºå¸¦è®¡ç®—å›¾ä¿¡æ¯
# ============================
print("\n2. æ¼”ç¤ºtensorå¦‚ä½•æºå¸¦è®¡ç®—å›¾ä¿¡æ¯")
print("-" * 50)

def demonstrate_tensor_graph_storage():
    """æ¼”ç¤ºtensorå¦‚ä½•å­˜å‚¨è®¡ç®—å›¾ä¿¡æ¯"""
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¶å­èŠ‚ç‚¹
    x = torch.tensor([2.0], requires_grad=True)
    y = torch.tensor([3.0], requires_grad=True)
    
    print("ğŸŒ± å¶å­èŠ‚ç‚¹ï¼ˆç”¨æˆ·åˆ›å»ºçš„tensorï¼‰:")
    print(f"x = {x}, grad_fn = {x.grad_fn}, is_leaf = {x.is_leaf}")
    print(f"y = {y}, grad_fn = {y.grad_fn}, is_leaf = {y.is_leaf}")
    print()
    
    # è¿›è¡Œè®¡ç®—ï¼Œè§‚å¯Ÿè®¡ç®—å›¾çš„æ„å»º
    print("ğŸ”„ è®¡ç®—è¿‡ç¨‹ä¸­è®¡ç®—å›¾çš„æ„å»º:")
    
    # ç¬¬ä¸€æ­¥è®¡ç®—
    z1 = x * y
    print(f"z1 = x * y = {z1}")
    print(f"z1.grad_fn = {z1.grad_fn}")
    print(f"z1.is_leaf = {z1.is_leaf}")
    print()
    
    # ç¬¬äºŒæ­¥è®¡ç®—
    z2 = z1 + 10
    print(f"z2 = z1 + 10 = {z2}")
    print(f"z2.grad_fn = {z2.grad_fn}")
    print(f"z2.is_leaf = {z2.is_leaf}")
    print()
    
    # ç¬¬ä¸‰æ­¥è®¡ç®—
    loss = z2 ** 2
    print(f"loss = z2 ** 2 = {loss}")
    print(f"loss.grad_fn = {loss.grad_fn}")
    print(f"loss.is_leaf = {loss.is_leaf}")
    print()
    
    return loss, x, y

loss_demo, x_demo, y_demo = demonstrate_tensor_graph_storage()

# ============================
# 3. æ·±å…¥æ¢ç´¢grad_fné“¾æ¡
# ============================
print("\n3. æ·±å…¥æ¢ç´¢grad_fné“¾æ¡")
print("-" * 50)

def explore_grad_fn_chain(tensor):
    """é€’å½’æ¢ç´¢grad_fné“¾æ¡"""
    
    print("ğŸ” ä»losså¼€å§‹é€’å½’æ¢ç´¢è®¡ç®—å›¾:")
    
    def recursive_explore(node, depth=0):
        indent = "  " * depth
        if hasattr(node, 'next_functions'):
            print(f"{indent}èŠ‚ç‚¹: {node}")
            print(f"{indent}ç±»å‹: {type(node).__name__}")
            
            # æ¢ç´¢ä¸‹ä¸€å±‚èŠ‚ç‚¹
            for i, (next_fn, _) in enumerate(node.next_functions):
                if next_fn is not None:
                    print(f"{indent}  â””â”€ ä¸‹çº§èŠ‚ç‚¹ {i}:")
                    recursive_explore(next_fn, depth + 2)
                else:
                    print(f"{indent}  â””â”€ å¶å­èŠ‚ç‚¹ {i} (None)")
        else:
            print(f"{indent}å¶å­èŠ‚ç‚¹: {node}")
    
    if tensor.grad_fn is not None:
        recursive_explore(tensor.grad_fn)
    else:
        print("è¿™æ˜¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ²¡æœ‰grad_fn")

explore_grad_fn_chain(loss_demo)

# ============================
# 4. æ¨¡æ‹Ÿbackward()çš„å·¥ä½œè¿‡ç¨‹
# ============================
print("\n4. æ¨¡æ‹Ÿbackward()çš„å·¥ä½œè¿‡ç¨‹")
print("-" * 50)

def simulate_backward_process():
    """æ¨¡æ‹Ÿbackward()å¦‚ä½•éå†è®¡ç®—å›¾"""
    
    # åˆ›å»ºç®€å•çš„è®¡ç®—å›¾
    a = torch.tensor([2.0], requires_grad=True)
    b = torch.tensor([3.0], requires_grad=True)
    
    c = a * b      # MulBackward
    d = c + 1      # AddBackward  
    loss = d ** 2  # PowBackward
    
    print("ğŸ“Š æ„å»ºçš„è®¡ç®—å›¾:")
    print(f"loss = (a * b + 1) ** 2")
    print(f"a = {a.item()}, b = {b.item()}")
    print(f"loss = {loss.item()}")
    print()
    
    print("ğŸ”„ backward()çš„éå†è¿‡ç¨‹ï¼ˆæ¨¡æ‹Ÿï¼‰:")
    
    # æ¨¡æ‹Ÿbackwardè¿‡ç¨‹
    print("1. ä»losså¼€å§‹ (grad = 1.0)")
    loss_grad = 1.0
    
    print("2. PowBackward: d_loss/d_d = 2 * d")
    d_value = (a * b + 1).item()
    d_grad = 2 * d_value * loss_grad
    print(f"   dçš„æ¢¯åº¦ = {d_grad}")
    
    print("3. AddBackward: d_d/d_c = 1")
    c_grad = 1.0 * d_grad
    print(f"   cçš„æ¢¯åº¦ = {c_grad}")
    
    print("4. MulBackward: d_c/d_a = b, d_c/d_b = a")
    a_grad = b.item() * c_grad
    b_grad = a.item() * c_grad
    print(f"   açš„æ¢¯åº¦ = {a_grad}")
    print(f"   bçš„æ¢¯åº¦ = {b_grad}")
    
    # éªŒè¯æˆ‘ä»¬çš„æ‰‹åŠ¨è®¡ç®—
    print("\nâœ… éªŒè¯è®¡ç®—ç»“æœ:")
    loss.backward()
    print(f"PyTorchè®¡ç®—çš„aæ¢¯åº¦: {a.grad.item()}")
    print(f"PyTorchè®¡ç®—çš„bæ¢¯åº¦: {b.grad.item()}")
    print(f"æ‰‹åŠ¨è®¡ç®—æ˜¯å¦æ­£ç¡®: a={abs(a.grad.item() - a_grad) < 1e-6}, b={abs(b.grad.item() - b_grad) < 1e-6}")

simulate_backward_process()

# ============================
# 5. åœ¨ä½ çš„ç¥ç»ç½‘ç»œä¸­çš„è®¡ç®—å›¾
# ============================
print("\n5. åœ¨ä½ çš„ç¥ç»ç½‘ç»œä¸­çš„è®¡ç®—å›¾")
print("-" * 50)

def analyze_neural_network_graph():
    """åˆ†æç¥ç»ç½‘ç»œä¸­çš„è®¡ç®—å›¾ç»“æ„"""
    
    # é‡ç°ä½ çš„æ¨¡å‹
    class SimpleClassifier(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(SimpleClassifier, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, output_size)
            self.dropout = nn.Dropout(0.2)
            
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.dropout(x)
            x = F.relu(self.fc2(x))
            x = self.dropout(x)
            x = self.fc3(x)
            return x
    
    model = SimpleClassifier(2, 4, 2)
    criterion = nn.CrossEntropyLoss()
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    x = torch.randn(3, 2)
    target = torch.tensor([0, 1, 0])
    
    print("ğŸ§  ç¥ç»ç½‘ç»œè®¡ç®—å›¾åˆ†æ:")
    
    # å‰å‘ä¼ æ’­çš„æ¯ä¸€æ­¥
    print("å‰å‘ä¼ æ’­è¿‡ç¨‹:")
    print(f"è¾“å…¥ x: {x.shape}, grad_fn = {x.grad_fn}")
    
    h1 = model.fc1(x)
    print(f"fc1è¾“å‡º: {h1.shape}, grad_fn = {type(h1.grad_fn).__name__}")
    
    a1 = F.relu(h1)
    print(f"relu1è¾“å‡º: {a1.shape}, grad_fn = {type(a1.grad_fn).__name__}")
    
    d1 = model.dropout(a1)
    print(f"dropout1è¾“å‡º: {d1.shape}, grad_fn = {type(d1.grad_fn).__name__}")
    
    h2 = model.fc2(d1)
    print(f"fc2è¾“å‡º: {h2.shape}, grad_fn = {type(h2.grad_fn).__name__}")
    
    a2 = F.relu(h2)
    print(f"relu2è¾“å‡º: {a2.shape}, grad_fn = {type(a2.grad_fn).__name__}")
    
    d2 = model.dropout(a2)
    print(f"dropout2è¾“å‡º: {d2.shape}, grad_fn = {type(d2.grad_fn).__name__}")
    
    output = model.fc3(d2)
    print(f"æœ€ç»ˆè¾“å‡º: {output.shape}, grad_fn = {type(output.grad_fn).__name__}")
    
    loss = criterion(output, target)
    print(f"æŸå¤±: {loss.shape}, grad_fn = {type(loss.grad_fn).__name__}")
    print()
    
    # æ˜¾ç¤ºè®¡ç®—å›¾çš„æ·±åº¦
    def count_graph_depth(tensor):
        if tensor.grad_fn is None:
            return 0
        
        max_depth = 0
        if hasattr(tensor.grad_fn, 'next_functions'):
            for next_fn, _ in tensor.grad_fn.next_functions:
                if next_fn is not None:
                    # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ï¼Œä¸é€’å½’è®¡ç®—
                    max_depth = max(max_depth, 1)
        return max_depth + 1
    
    depth = count_graph_depth(loss)
    print(f"ğŸ“ è®¡ç®—å›¾æ·±åº¦: {depth} (ç®€åŒ–è®¡ç®—)")
    
    return loss

loss_nn = analyze_neural_network_graph()

# ============================
# 6. è®¡ç®—å›¾çš„å†…å­˜ç®¡ç†
# ============================
print("\n6. è®¡ç®—å›¾çš„å†…å­˜ç®¡ç†")
print("-" * 50)

def demonstrate_graph_memory():
    """æ¼”ç¤ºè®¡ç®—å›¾çš„å†…å­˜ç®¡ç†"""
    
    x = torch.tensor([1.0], requires_grad=True)
    
    print("ğŸ—„ï¸ è®¡ç®—å›¾çš„å†…å­˜ç®¡ç†:")
    
    # åˆ›å»ºè®¡ç®—å›¾
    y = x ** 2
    z = y * 3
    loss = z + 10
    
    print(f"lossåˆ›å»ºåï¼Œloss.grad_fnå­˜åœ¨: {loss.grad_fn is not None}")
    print(f"è®¡ç®—å›¾é“¾æ¡: loss -> {type(loss.grad_fn).__name__} -> {type(y.grad_fn).__name__} -> ...")
    
    # backwardåè®¡ç®—å›¾çš„çŠ¶æ€
    loss.backward()
    print(f"backwardåï¼Œloss.grad_fnå­˜åœ¨: {loss.grad_fn is not None}")
    
    # å†æ¬¡backwardä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
    try:
        loss.backward()
    except RuntimeError as e:
        print(f"å†æ¬¡backwardå¤±è´¥: {e}")
    
    # retain_graph=Trueçš„æƒ…å†µ
    y2 = x ** 2
    z2 = y2 * 3  
    loss2 = z2 + 10
    
    print(f"\nä½¿ç”¨retain_graph=True:")
    loss2.backward(retain_graph=True)
    print(f"ç¬¬ä¸€æ¬¡backwardåï¼Œloss2.grad_fnå­˜åœ¨: {loss2.grad_fn is not None}")
    
    loss2.backward(retain_graph=True)
    print(f"ç¬¬äºŒæ¬¡backwardæˆåŠŸï¼Œä½†æ¢¯åº¦ä¼šç´¯ç§¯!")
    print(f"xçš„ç´¯ç§¯æ¢¯åº¦: {x.grad}")

demonstrate_graph_memory()

# ============================
# 7. åŠ¨æ€è®¡ç®—å›¾ vs é™æ€è®¡ç®—å›¾
# ============================
print("\n7. åŠ¨æ€è®¡ç®—å›¾ vs é™æ€è®¡ç®—å›¾")
print("-" * 50)

dynamic_vs_static = """
ğŸ”„ PyTorchçš„åŠ¨æ€è®¡ç®—å›¾ç‰¹ç‚¹:

ğŸŸ¢ **åŠ¨æ€è®¡ç®—å›¾ (PyTorch)**:
â€¢ æ¯æ¬¡å‰å‘ä¼ æ’­éƒ½é‡æ–°æ„å»ºè®¡ç®—å›¾
â€¢ æ”¯æŒæ§åˆ¶æµï¼ˆifã€forã€whileï¼‰
â€¢ å›¾ç»“æ„å¯ä»¥åœ¨è¿è¡Œæ—¶æ”¹å˜
â€¢ ä¾¿äºè°ƒè¯•ï¼Œå¯ä»¥ä½¿ç”¨Pythonè°ƒè¯•å™¨

ğŸ”µ **é™æ€è®¡ç®—å›¾ (TensorFlow 1.x)**:
â€¢ å…ˆå®šä¹‰å›¾ç»“æ„ï¼Œå†æ‰§è¡Œè®¡ç®—
â€¢ å›¾ç»“æ„å›ºå®šï¼Œä¸èƒ½åœ¨è¿è¡Œæ—¶æ”¹å˜
â€¢ ä¼˜åŒ–æ›´å……åˆ†ï¼Œæ‰§è¡Œæ•ˆç‡æ›´é«˜
â€¢ éš¾ä»¥è°ƒè¯•

ğŸ’¡ **PyTorchåŠ¨æ€å›¾çš„å®ç°**:
æ¯æ¬¡è°ƒç”¨.backward()æ—¶ï¼š
1. ä»loss tensorå¼€å§‹
2. é€šè¿‡grad_fné€’å½’éå†æ•´ä¸ªå›¾
3. åº”ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
4. é»˜è®¤é‡Šæ”¾å›¾å†…å­˜ï¼ˆé™¤éretain_graph=Trueï¼‰

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆloss.backward()ä¸éœ€è¦æ˜¾å¼çš„å›¾å¼•ç”¨ï¼
"""
print(dynamic_vs_static)

# ============================
# 8. è‡ªå®šä¹‰å‡½æ•°çš„è®¡ç®—å›¾
# ============================
print("\n8. è‡ªå®šä¹‰å‡½æ•°çš„è®¡ç®—å›¾")
print("-" * 50)

class CustomSquare(torch.autograd.Function):
    """è‡ªå®šä¹‰å¹³æ–¹å‡½æ•°ï¼Œå±•ç¤ºå¦‚ä½•é›†æˆåˆ°è®¡ç®—å›¾"""
    
    @staticmethod
    def forward(ctx, input):
        """å‰å‘ä¼ æ’­"""
        ctx.save_for_backward(input)  # ä¿å­˜ç”¨äºåå‘ä¼ æ’­çš„tensor
        return input ** 2
    
    @staticmethod
    def backward(ctx, grad_output):
        """åå‘ä¼ æ’­"""
        input, = ctx.saved_tensors
        return grad_output * 2 * input

def demonstrate_custom_function():
    """æ¼”ç¤ºè‡ªå®šä¹‰å‡½æ•°å¦‚ä½•é›†æˆåˆ°è®¡ç®—å›¾"""
    
    print("ğŸ› ï¸ è‡ªå®šä¹‰å‡½æ•°çš„è®¡ç®—å›¾é›†æˆ:")
    
    x = torch.tensor([3.0], requires_grad=True)
    
    # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
    y = CustomSquare.apply(x)
    loss = y.sum()
    
    print(f"x = {x}")
    print(f"y = CustomSquare(x) = {y}")
    print(f"y.grad_fn = {y.grad_fn}")
    print(f"loss.grad_fn = {loss.grad_fn}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    print(f"xçš„æ¢¯åº¦ = {x.grad} (åº”è¯¥æ˜¯ 2*3 = 6)")

demonstrate_custom_function()

# ============================
# 9. æ€»ç»“
# ============================
print("\n" + "="*70)
print("æ€»ç»“")
print("="*70)

summary = """
ğŸ“ loss.backward() å¦‚ä½•æ‰¾åˆ°è®¡ç®—å›¾çš„å®Œæ•´æœºåˆ¶ï¼š

ğŸ”— **æ ¸å¿ƒåŸç†**ï¼š
   â€¢ è®¡ç®—å›¾ä¸æ˜¯ç‹¬ç«‹çš„æ•°æ®ç»“æ„
   â€¢ è€Œæ˜¯åˆ†å¸ƒå¼å­˜å‚¨åœ¨æ¯ä¸ªtensorçš„grad_fnå±æ€§ä¸­
   â€¢ loss tensoré€šè¿‡grad_fné“¾å¯ä»¥è®¿é—®æ•´ä¸ªè®¡ç®—å›¾

ğŸ“Š **tensorçš„å…³é”®å±æ€§**ï¼š
   â€¢ grad_fn: æŒ‡å‘åˆ›å»ºè¯¥tensorçš„å‡½æ•°èŠ‚ç‚¹
   â€¢ requires_grad: æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦
   â€¢ is_leaf: æ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹ï¼ˆç”¨æˆ·åˆ›å»ºï¼‰

ğŸ”„ **backward()çš„å·¥ä½œæµç¨‹**ï¼š
   1. ä»loss.grad_fnå¼€å§‹
   2. é€’å½’è®¿é—®next_functions
   3. å¯¹æ¯ä¸ªèŠ‚ç‚¹åº”ç”¨é“¾å¼æ³•åˆ™
   4. å°†æ¢¯åº¦ç´¯ç§¯åˆ°å¶å­èŠ‚ç‚¹çš„.gradå±æ€§

ğŸ’¡ **åœ¨ä½ çš„ä»£ç ä¸­**ï¼š
   ```python
   outputs = model(data)           # æ„å»ºè®¡ç®—å›¾ï¼Œå­˜å‚¨åœ¨outputs.grad_fnä¸­
   loss = criterion(outputs, target)  # æ‰©å±•è®¡ç®—å›¾ï¼Œå­˜å‚¨åœ¨loss.grad_fnä¸­
   loss.backward()                 # ä»loss.grad_fnå¼€å§‹éå†æ•´ä¸ªå›¾
   ```

ğŸ¯ **å…³é”®ç†è§£**ï¼š
   â€¢ loss.backward()ä¸éœ€è¦å¤–éƒ¨å›¾å¼•ç”¨
   â€¢ å› ä¸ºlossæœ¬èº«å°±"æºå¸¦"äº†æ•´ä¸ªè®¡ç®—å›¾çš„ä¿¡æ¯
   â€¢ é€šè¿‡grad_fné“¾ï¼Œå¯ä»¥æ‰¾åˆ°æ‰€æœ‰éœ€è¦æ¢¯åº¦çš„å‚æ•°
   â€¢ è¿™å°±æ˜¯PyTorchåŠ¨æ€è®¡ç®—å›¾çš„ç²¾å¦™è®¾è®¡ï¼

ğŸš€ **åŠ¨æ€å›¾çš„ä¼˜åŠ¿**ï¼š
   â€¢ æ¯æ¬¡å‰å‘ä¼ æ’­é‡æ–°æ„å»ºï¼Œæ”¯æŒåŠ¨æ€æ§åˆ¶æµ
   â€¢ ä¾¿äºè°ƒè¯•ï¼Œå¯ä»¥æ£€æŸ¥ä¸­é—´ç»“æœ
   â€¢ è‡ªç„¶æ”¯æŒå˜é•¿åºåˆ—ç­‰åŠ¨æ€è¾“å…¥

è¿™ç§è®¾è®¡è®©PyTorchæ—¢å¼ºå¤§åˆçµæ´»ï¼ğŸ‰
"""
print(summary)

if __name__ == "__main__":
    print("\nğŸ‰ è®¡ç®—å›¾æœºåˆ¶è§£æå®Œæˆï¼")