import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['PingFang SC', 'Hiragino Sans GB', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

print("=" * 70)
print("æ¢¯åº¦åœ¨ç¥ç»ç½‘ç»œä¸­çš„ä½œç”¨è¯¦è§£")
print("=" * 70)

# ============================
# 1. æ¢¯åº¦çš„æœ¬è´¨å«ä¹‰
# ============================
print("\n1. æ¢¯åº¦çš„æœ¬è´¨å«ä¹‰")
print("-" * 50)

gradient_concept = """
ğŸ¯ æ¢¯åº¦çš„æœ¬è´¨ï¼š

ğŸ“ **æ•°å­¦å®šä¹‰**ï¼š
â€¢ æ¢¯åº¦æ˜¯å‡½æ•°åœ¨æŸç‚¹å¤„çš„å˜åŒ–ç‡ï¼ˆå¯¼æ•°ï¼‰
â€¢ å¯¹äºå¤šå˜é‡å‡½æ•°ï¼Œæ¢¯åº¦æ˜¯åå¯¼æ•°ç»„æˆçš„å‘é‡
â€¢ æ¢¯åº¦æŒ‡å‘å‡½æ•°å€¼å¢é•¿æœ€å¿«çš„æ–¹å‘

ğŸ§  **åœ¨ç¥ç»ç½‘ç»œä¸­çš„å«ä¹‰**ï¼š
â€¢ æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ•æ„Ÿåº¦
â€¢ æ­£æ¢¯åº¦ï¼šå¢åŠ å‚æ•°ä¼šå¢åŠ æŸå¤±
â€¢ è´Ÿæ¢¯åº¦ï¼šå¢åŠ å‚æ•°ä¼šå‡å°‘æŸå¤±
â€¢ æ¢¯åº¦å¤§å°ï¼šè¡¨ç¤ºæ•æ„Ÿç¨‹åº¦

ğŸ›ï¸ **ç›´è§‚ç†è§£**ï¼š
æƒ³è±¡ä½ åœ¨å±±ä¸Šè¿·é›¾ä¸­å¯»æ‰¾æœ€ä½ç‚¹ï¼š
â€¢ æ¢¯åº¦ = å½“å‰ä½ç½®çš„å¡åº¦æ–¹å‘
â€¢ è´Ÿæ¢¯åº¦æ–¹å‘ = ä¸‹å±±æœ€å¿«çš„æ–¹å‘
â€¢ æ¢¯åº¦å¤§å° = å¡åº¦é™¡å³­ç¨‹åº¦
"""
print(gradient_concept)

# ============================
# 2. æ¢¯åº¦åœ¨è®­ç»ƒå¾ªç¯ä¸­çš„å…·ä½“ä½œç”¨
# ============================
print("\n2. æ¢¯åº¦åœ¨è®­ç»ƒå¾ªç¯ä¸­çš„å…·ä½“ä½œç”¨")
print("-" * 50)

def demonstrate_gradient_flow():
    """æ¼”ç¤ºæ¢¯åº¦åœ¨è®­ç»ƒä¸­çš„æµåŠ¨"""
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(1, 1, bias=False)
            # åˆå§‹åŒ–æƒé‡ä¸ºä¸€ä¸ªå·²çŸ¥å€¼
            self.linear.weight.data = torch.tensor([[2.0]])
        
        def forward(self, x):
            return self.linear(x)
    
    model = SimpleModel()
    
    # å‡†å¤‡ç®€å•æ•°æ®ï¼šy = 3x (æˆ‘ä»¬æƒ³è®©æ¨¡å‹å­¦åˆ°æƒé‡ä¸º3)
    x = torch.tensor([[1.0], [2.0], [3.0]])
    y_true = torch.tensor([[3.0], [6.0], [9.0]])
    
    criterion = nn.MSELoss()
    
    print("ğŸ“Š æ¢¯åº¦è®¡ç®—æ¼”ç¤ºï¼š")
    print(f"ç›®æ ‡å…³ç³»: y = 3x")
    print(f"æ¨¡å‹åˆå§‹æƒé‡: {model.linear.weight.item():.3f}")
    print(f"è®­ç»ƒæ•°æ®: x={x.flatten()}, y_true={y_true.flatten()}")
    print()
    
    # å‰å‘ä¼ æ’­
    y_pred = model(x)
    loss = criterion(y_pred, y_true)
    
    print(f"åˆå§‹é¢„æµ‹: {y_pred.flatten()}")
    print(f"åˆå§‹æŸå¤±: {loss.item():.3f}")
    
    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    loss.backward()
    
    gradient = model.linear.weight.grad.item()
    print(f"è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦: {gradient:.3f}")
    print()
    
    # è§£é‡Šæ¢¯åº¦çš„å«ä¹‰
    print("ğŸ” æ¢¯åº¦å«ä¹‰è§£æï¼š")
    print(f"â€¢ æ¢¯åº¦ = {gradient:.3f} < 0")
    print("â€¢ è´Ÿæ¢¯åº¦æ„å‘³ç€ï¼šå‡å°‘æƒé‡ä¼šå¢åŠ æŸå¤±ï¼Œå¢åŠ æƒé‡ä¼šå‡å°‘æŸå¤±")
    print("â€¢ æ‰€ä»¥æˆ‘ä»¬åº”è¯¥å‘æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°æƒé‡")
    print(f"â€¢ å½“å‰æƒé‡ {model.linear.weight.item():.3f} åº”è¯¥å¢åŠ åˆ°æ¥è¿‘ç›®æ ‡å€¼ 3.0")
    
    return model, gradient

model, gradient = demonstrate_gradient_flow()

# ============================
# 3. æ¢¯åº¦ä¸‹é™çš„å·¥ä½œåŸç†
# ============================
print("\n3. æ¢¯åº¦ä¸‹é™çš„å·¥ä½œåŸç†")
print("-" * 50)

def visualize_gradient_descent():
    """å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹"""
    
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„äºŒæ¬¡å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°
    def loss_function(w):
        return (w - 3)**2  # æœ€å°å€¼åœ¨ w = 3
    
    def loss_gradient(w):
        return 2 * (w - 3)  # æ¢¯åº¦
    
    # æ¢¯åº¦ä¸‹é™å‚æ•°
    learning_rate = 0.1
    initial_w = 0.0
    num_steps = 20
    
    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    w_history = [initial_w]
    loss_history = [loss_function(initial_w)]
    gradient_history = [loss_gradient(initial_w)]
    
    w = initial_w
    print("ğŸ“ˆ æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ï¼š")
    print(f"{'æ­¥éª¤':>4} {'æƒé‡':>8} {'æŸå¤±':>8} {'æ¢¯åº¦':>8} {'æ›´æ–°':>12}")
    print("-" * 50)
    
    for step in range(num_steps):
        grad = loss_gradient(w)
        update = -learning_rate * grad  # è´Ÿæ¢¯åº¦æ–¹å‘
        w_new = w + update
        loss_val = loss_function(w)
        
        print(f"{step:4d} {w:8.3f} {loss_val:8.3f} {grad:8.3f} {update:+8.3f}")
        
        w = w_new
        w_history.append(w)
        loss_history.append(loss_function(w))
        gradient_history.append(loss_gradient(w))
        
        # å¦‚æœæ¢¯åº¦å¾ˆå°ï¼Œè¯´æ˜æ¥è¿‘æœ€ä¼˜è§£
        if abs(grad) < 0.01:
            print(f"âœ… æ”¶æ•›ï¼æœ€ç»ˆæƒé‡: {w:.3f}, ç›®æ ‡æƒé‡: 3.0")
            break
    
    # å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
    try:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±å‡½æ•°æ›²çº¿
        w_range = np.linspace(-1, 5, 100)
        loss_range = [(w - 3)**2 for w in w_range]
        
        ax1.plot(w_range, loss_range, 'b-', label='æŸå¤±å‡½æ•° L(w)=(w-3)Â²', linewidth=2)
        ax1.plot(w_history, loss_history, 'ro-', label='æ¢¯åº¦ä¸‹é™è·¯å¾„', markersize=5)
        ax1.set_xlabel('æƒé‡ w')
        ax1.set_ylabel('æŸå¤± L(w)')
        ax1.set_title('æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿‡ç¨‹')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æŸå¤±éšè®­ç»ƒæ­¥éª¤çš„å˜åŒ–
        ax2.plot(loss_history, 'g-o', linewidth=2, markersize=4)
        ax2.set_xlabel('è®­ç»ƒæ­¥éª¤')
        ax2.set_ylabel('æŸå¤±å€¼')
        ax2.set_title('æŸå¤±éšè®­ç»ƒæ­¥éª¤çš„ä¸‹é™')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/Users/xiang/Documents/GitHub/NewStart/NewStart/gradient_descent.png', 
                    dpi=300, bbox_inches='tight')
        print(f"\næ¢¯åº¦ä¸‹é™å¯è§†åŒ–å·²ä¿å­˜")
        
    except Exception as e:
        print(f"å¯è§†åŒ–å‡ºé”™: {e}")
    
    return w_history, loss_history

w_hist, loss_hist = visualize_gradient_descent()

# ============================
# 4. åœ¨ä½ çš„PyTorchä»£ç ä¸­çš„æ¢¯åº¦æµåŠ¨
# ============================
print("\n4. åœ¨ä½ çš„PyTorchä»£ç ä¸­çš„æ¢¯åº¦æµåŠ¨")
print("-" * 50)

def analyze_pytorch_gradient_flow():
    """åˆ†æä½ çš„PyTorchä»£ç ä¸­çš„æ¢¯åº¦æµåŠ¨"""
    
    # é‡ç°ä½ çš„æ¨¡å‹ç»“æ„
    class SimpleClassifier(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(SimpleClassifier, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, output_size)
            self.dropout = nn.Dropout(0.2)
            
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.dropout(x)
            x = F.relu(self.fc2(x))
            x = self.dropout(x)
            x = self.fc3(x)
            return x
    
    model = SimpleClassifier(2, 4, 2)  # ç®€åŒ–ç‰ˆæœ¬ä¾¿äºæ¼”ç¤º
    criterion = nn.CrossEntropyLoss()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    x = torch.randn(3, 2)  # 3ä¸ªæ ·æœ¬ï¼Œ2ä¸ªç‰¹å¾
    y = torch.tensor([0, 1, 0])  # 3ä¸ªæ ‡ç­¾
    
    print("ğŸ”„ PyTorchè®­ç»ƒæ­¥éª¤ä¸­çš„æ¢¯åº¦ï¼š")
    print()
    
    # å‰å‘ä¼ æ’­
    print("1ï¸âƒ£ å‰å‘ä¼ æ’­ï¼š")
    print(f"   è¾“å…¥ x: {x.shape}")
    output = model(x)
    print(f"   è¾“å‡º output: {output.shape}")
    loss = criterion(output, y)
    print(f"   æŸå¤± loss: {loss.item():.4f}")
    print()
    
    # æŸ¥çœ‹å‚æ•°åˆå§‹çŠ¶æ€
    print("2ï¸âƒ£ åå‘ä¼ æ’­å‰çš„å‚æ•°çŠ¶æ€ï¼š")
    for name, param in model.named_parameters():
        if param.grad is not None:
            print(f"   {name}: grad = {param.grad.norm().item():.4f}")
        else:
            print(f"   {name}: grad = None")
    print()
    
    # åå‘ä¼ æ’­
    print("3ï¸âƒ£ åå‘ä¼ æ’­ï¼š")
    loss.backward()
    print("   è®¡ç®—æ¢¯åº¦å®Œæˆï¼")
    print()
    
    # æŸ¥çœ‹è®¡ç®—å‡ºçš„æ¢¯åº¦
    print("4ï¸âƒ£ è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼š")
    total_grad_norm = 0
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            total_grad_norm += grad_norm**2
            print(f"   {name:12}: æ¢¯åº¦èŒƒæ•° = {grad_norm:.6f}")
    
    total_grad_norm = np.sqrt(total_grad_norm)
    print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
    print()
    
    # æ¨¡æ‹Ÿå‚æ•°æ›´æ–°
    print("5ï¸âƒ£ å‚æ•°æ›´æ–° (å­¦ä¹ ç‡ = 0.01)ï¼š")
    lr = 0.01
    with torch.no_grad():
        for name, param in model.named_parameters():
            if param.grad is not None:
                old_param = param.clone()
                param -= lr * param.grad
                update_size = (old_param - param).norm().item()
                print(f"   {name:12}: æ›´æ–°å¤§å° = {update_size:.6f}")

analyze_pytorch_gradient_flow()

# ============================
# 5. æ¢¯åº¦çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
# ============================
print("\n5. æ¢¯åº¦çš„å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ")
print("-" * 50)

gradient_problems = """
ğŸš¨ æ¢¯åº¦ç›¸å…³çš„å¸¸è§é—®é¢˜ï¼š

1ï¸âƒ£ **æ¢¯åº¦æ¶ˆå¤± (Gradient Vanishing)**ï¼š
   â€¢ é—®é¢˜: æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ¸å˜å°ï¼Œè¶‹è¿‘äº0
   â€¢ åŸå› : æ¿€æ´»å‡½æ•°å¯¼æ•°å°ï¼ˆå¦‚Sigmoidï¼‰ã€ç½‘ç»œè¿‡æ·±
   â€¢ ç—‡çŠ¶: å‰å‡ å±‚å‚æ•°å‡ ä¹ä¸æ›´æ–°ï¼Œè®­ç»ƒç¼“æ…¢
   â€¢ è§£å†³: ä½¿ç”¨ReLUã€æ®‹å·®è¿æ¥ã€BatchNorm

2ï¸âƒ£ **æ¢¯åº¦çˆ†ç‚¸ (Gradient Exploding)**ï¼š
   â€¢ é—®é¢˜: æ¢¯åº¦å˜å¾—éå¸¸å¤§ï¼Œå¯¼è‡´å‚æ•°æ›´æ–°è¿‡å¤§
   â€¢ åŸå› : æƒé‡åˆå§‹åŒ–ä¸å½“ã€å­¦ä¹ ç‡è¿‡å¤§
   â€¢ ç—‡çŠ¶: æŸå¤±çªç„¶å¢å¤§ã€NaNå€¼å‡ºç°
   â€¢ è§£å†³: æ¢¯åº¦è£å‰ªã€é™ä½å­¦ä¹ ç‡ã€æƒé‡åˆå§‹åŒ–

3ï¸âƒ£ **æ­»äº¡ReLU (Dead ReLU)**ï¼š
   â€¢ é—®é¢˜: ReLUç¥ç»å…ƒè¾“å‡ºæ°¸è¿œä¸º0ï¼Œæ¢¯åº¦ä¸º0
   â€¢ åŸå› : è´Ÿå€¼è¾“å…¥å¯¼è‡´ReLUè¾“å‡º0ï¼Œæ— æ³•æ¢å¤
   â€¢ ç—‡çŠ¶: éƒ¨åˆ†ç¥ç»å…ƒä¸æ›´æ–°
   â€¢ è§£å†³: ä½¿ç”¨Leaky ReLUã€é™ä½å­¦ä¹ ç‡

4ï¸âƒ£ **æ¢¯åº¦å™ªå£°**ï¼š
   â€¢ é—®é¢˜: å°æ‰¹é‡è®­ç»ƒå¯¼è‡´æ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®
   â€¢ åŸå› : æ‰¹é‡å¤§å°å¤ªå°
   â€¢ ç—‡çŠ¶: è®­ç»ƒä¸ç¨³å®šï¼Œéœ‡è¡
   â€¢ è§£å†³: å¢å¤§æ‰¹é‡ã€ä½¿ç”¨åŠ¨é‡ä¼˜åŒ–å™¨
"""
print(gradient_problems)

# ============================
# 6. æ¢¯åº¦ç›‘æ§å’Œè°ƒè¯•
# ============================
print("\n6. æ¢¯åº¦ç›‘æ§å’Œè°ƒè¯•æŠ€å·§")
print("-" * 50)

def gradient_monitoring_demo():
    """æ¼”ç¤ºæ¢¯åº¦ç›‘æ§æŠ€å·§"""
    
    # åˆ›å»ºä¸€ä¸ªå¯èƒ½æœ‰æ¢¯åº¦é—®é¢˜çš„æ·±å±‚ç½‘ç»œ
    class DeepModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = nn.ModuleList([
                nn.Linear(10, 10) for _ in range(10)  # 10å±‚
            ])
            self.output = nn.Linear(10, 1)
        
        def forward(self, x):
            for layer in self.layers:
                x = torch.sigmoid(layer(x))  # ä½¿ç”¨Sigmoidï¼ˆå®¹æ˜“æ¢¯åº¦æ¶ˆå¤±ï¼‰
            return self.output(x)
    
    model = DeepModel()
    criterion = nn.MSELoss()
    
    # åˆ›å»ºæ•°æ®
    x = torch.randn(32, 10)
    y = torch.randn(32, 1)
    
    # å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    
    print("ğŸ” æ¢¯åº¦ç›‘æ§æŠ€å·§ï¼š")
    print()
    
    # 1. æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
    print("1ï¸âƒ£ å„å±‚æ¢¯åº¦èŒƒæ•°ï¼š")
    layer_grads = []
    for i, layer in enumerate(model.layers):
        if layer.weight.grad is not None:
            grad_norm = layer.weight.grad.norm().item()
            layer_grads.append(grad_norm)
            print(f"   Layer {i:2d}: {grad_norm:.6f}")
    
    # 2. æ¢¯åº¦æ¯”ç‡åˆ†æ
    print("\n2ï¸âƒ£ æ¢¯åº¦æ¶ˆå¤±åˆ†æï¼š")
    if len(layer_grads) > 1:
        ratios = [layer_grads[i+1]/layer_grads[i] if layer_grads[i] > 0 else 0 
                 for i in range(len(layer_grads)-1)]
        avg_ratio = np.mean(ratios)
        print(f"   å¹³å‡æ¢¯åº¦æ¯”ç‡: {avg_ratio:.6f}")
        if avg_ratio < 0.1:
            print("   âš ï¸  æ¢¯åº¦æ¶ˆå¤±é£é™©ï¼")
        elif avg_ratio > 10:
            print("   âš ï¸  æ¢¯åº¦çˆ†ç‚¸é£é™©ï¼")
        else:
            print("   âœ… æ¢¯åº¦ä¼ æ’­æ­£å¸¸")
    
    # 3. é›¶æ¢¯åº¦æ£€æµ‹
    print("\n3ï¸âƒ£ é›¶æ¢¯åº¦æ£€æµ‹ï¼š")
    zero_grad_count = 0
    total_params = 0
    for param in model.parameters():
        if param.grad is not None:
            zero_count = (param.grad.abs() < 1e-8).sum().item()
            zero_grad_count += zero_count
            total_params += param.numel()
    
    zero_ratio = zero_grad_count / total_params
    print(f"   é›¶æ¢¯åº¦å‚æ•°æ¯”ä¾‹: {zero_ratio:.2%}")
    if zero_ratio > 0.5:
        print("   âš ï¸  è¿‡å¤šå‚æ•°æ¢¯åº¦æ¥è¿‘é›¶ï¼")

gradient_monitoring_demo()

# ============================
# 7. ä¼˜åŒ–å™¨ä¸æ¢¯åº¦çš„å…³ç³»
# ============================
print("\n7. ä¸åŒä¼˜åŒ–å™¨å¦‚ä½•ä½¿ç”¨æ¢¯åº¦")
print("-" * 50)

optimizer_explanation = """
ğŸ›ï¸ ä¼˜åŒ–å™¨å¦‚ä½•ä½¿ç”¨æ¢¯åº¦ï¼š

ğŸ“Š **SGD (éšæœºæ¢¯åº¦ä¸‹é™)**ï¼š
   â€¢ æ›´æ–°å…¬å¼: Î¸ = Î¸ - lr Ã— âˆ‡Î¸
   â€¢ ç›´æ¥ä½¿ç”¨æ¢¯åº¦è¿›è¡Œæ›´æ–°
   â€¢ ç®€å•ä½†å¯èƒ½éœ‡è¡

ğŸš€ **Adam (ä½ ä»£ç ä¸­ä½¿ç”¨çš„)**ï¼š
   â€¢ æ›´æ–°å…¬å¼: Î¸ = Î¸ - lr Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)
   â€¢ mÌ‚: æ¢¯åº¦çš„ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰
   â€¢ vÌ‚: æ¢¯åº¦çš„äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
   â€¢ ç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡

ğŸ’¨ **Momentum**ï¼š
   â€¢ æ›´æ–°å…¬å¼: Î¸ = Î¸ - (Î±Ã—v + lrÃ—âˆ‡Î¸)
   â€¢ åˆ©ç”¨å†å²æ¢¯åº¦ä¿¡æ¯åŠ é€Ÿæ”¶æ•›
   â€¢ å‡å°‘éœ‡è¡

ğŸ¯ **RMSprop**ï¼š
   â€¢ è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
   â€¢ å¯¹é¢‘ç¹æ›´æ–°çš„å‚æ•°é™ä½å­¦ä¹ ç‡

åœ¨ä½ çš„PyTorchä»£ç ä¸­ï¼š
optimizer = optim.Adam(model.parameters(), lr=0.001)
â€¢ Adamä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦çš„åŠ¨é‡å’Œè‡ªé€‚åº”è°ƒæ•´
â€¢ ä½ åªéœ€è¦è°ƒç”¨optimizer.step()å³å¯
"""
print(optimizer_explanation)

# ============================
# 8. æ€»ç»“
# ============================
print("\n" + "="*70)
print("æ¢¯åº¦ä½œç”¨æ€»ç»“")
print("="*70)

summary = """
ğŸ“ æ¢¯åº¦åœ¨ç¥ç»ç½‘ç»œä¸­çš„å…³é”®ä½œç”¨ï¼š

ğŸ§­ **1. æŒ‡å‘ä¼˜åŒ–æ–¹å‘**ï¼š
   â€¢ æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è°ƒæ•´å‚æ•°æ¥å‡å°‘æŸå¤±
   â€¢ è´Ÿæ¢¯åº¦æ–¹å‘æ˜¯æŸå¤±å‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹å‘

âš¡ **2. æ§åˆ¶å­¦ä¹ é€Ÿåº¦**ï¼š
   â€¢ æ¢¯åº¦å¤§å°å†³å®šå‚æ•°æ›´æ–°çš„å¹…åº¦
   â€¢ å¤§æ¢¯åº¦ â†’ å¤§æ›´æ–°ï¼Œå°æ¢¯åº¦ â†’ å°æ›´æ–°

ğŸ”„ **3. ä¼ é€’å­¦ä¹ ä¿¡å·**ï¼š
   â€¢ é€šè¿‡åå‘ä¼ æ’­ï¼Œæ¢¯åº¦ä»è¾“å‡ºå±‚ä¼ é€’åˆ°è¾“å…¥å±‚
   â€¢ æ¯ä¸€å±‚çš„å‚æ•°éƒ½èƒ½å¾—åˆ°ç›¸åº”çš„æ›´æ–°ä¿¡å·

ğŸ“Š **4. åœ¨ä½ çš„ä»£ç ä¸­**ï¼š
   ```python
   # è®¡ç®—æ¢¯åº¦
   loss.backward()           # æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•æ”¹è¿›
   
   # ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°  
   optimizer.step()          # Adamåˆ©ç”¨æ¢¯åº¦æ™ºèƒ½æ›´æ–°å‚æ•°
   
   # æ¸…é›¶æ¢¯åº¦å‡†å¤‡ä¸‹ä¸€è½®
   optimizer.zero_grad()     # é¿å…æ¢¯åº¦ç´¯ç§¯
   ```

ğŸ¯ **å…³é”®ç†è§£**ï¼š
â€¢ æ¢¯åº¦ = æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ•æ„Ÿåº¦
â€¢ ä¼˜åŒ–å™¨ = æ¢¯åº¦çš„æ™ºèƒ½ä½¿ç”¨æ–¹å¼
â€¢ æ¿€æ´»å‡½æ•° = æ¢¯åº¦æµåŠ¨çš„æ§åˆ¶å™¨
â€¢ å­¦ä¹ ç‡ = æ¢¯åº¦æ›´æ–°çš„ç¼©æ”¾å› å­

ğŸ’¡ **æœ€ä½³å®è·µ**ï¼š
âœ… ç›‘æ§æ¢¯åº¦èŒƒæ•°ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
âœ… é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰
âœ… ä½¿ç”¨åˆé€‚çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼‰
âœ… è®¾ç½®åˆç†çš„å­¦ä¹ ç‡
"""
print(summary)

if __name__ == "__main__":
    print("\nğŸ‰ æ¢¯åº¦ä½œç”¨è§£æå®Œæˆï¼")