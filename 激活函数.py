import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['PingFang SC', 'Hiragino Sans GB', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

print("=" * 70)
print("æ¿€æ´»å‡½æ•°è¯¦è§£ï¼šReLU åŠå…¶ä»–å¸¸ç”¨æ¿€æ´»å‡½æ•°")
print("=" * 70)

# ============================
# 1. ä»€ä¹ˆæ˜¯æ¿€æ´»å‡½æ•°ï¼Ÿ
# ============================
print("\n1. æ¿€æ´»å‡½æ•°çš„ä½œç”¨")
print("-" * 50)

activation_intro = """
ğŸ§  æ¿€æ´»å‡½æ•°çš„æ ¸å¿ƒä½œç”¨ï¼š

1. **å¼•å…¥éçº¿æ€§**: 
   - æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç¥ç»ç½‘ç»œç­‰ä»·äºå•å±‚çº¿æ€§å˜æ¢
   - æ¿€æ´»å‡½æ•°ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»

2. **æ§åˆ¶ä¿¡å·ä¼ é€’**:
   - å†³å®šç¥ç»å…ƒæ˜¯å¦è¢«"æ¿€æ´»"
   - æ§åˆ¶ä¿¡æ¯åœ¨ç½‘ç»œä¸­çš„æµåŠ¨

3. **æ•°å€¼ç¨³å®šæ€§**:
   - å°†è¾“å‡ºé™åˆ¶åœ¨åˆç†èŒƒå›´å†…
   - é¿å…æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±

åœ¨ä½ çš„PyTorchä»£ç ä¸­ï¼š
x = F.relu(self.fc1(x))  # ç¬¬ä¸€å±‚ååº”ç”¨ReLU
x = F.relu(self.fc2(x))  # ç¬¬äºŒå±‚ååº”ç”¨ReLU
x = self.fc3(x)          # è¾“å‡ºå±‚ä¸ç”¨æ¿€æ´»å‡½æ•°
"""
print(activation_intro)

# ============================
# 2. ReLU æ¿€æ´»å‡½æ•°è¯¦è§£
# ============================
print("\n2. ReLU (Rectified Linear Unit) è¯¦è§£")
print("-" * 50)

def relu_function(x):
    """ReLUå‡½æ•°çš„å®ç°"""
    return np.maximum(0, x)

def relu_derivative(x):
    """ReLUå‡½æ•°çš„å¯¼æ•°"""
    return (x > 0).astype(float)

# ReLUçš„æ•°å­¦å®šä¹‰
relu_math = """
ğŸ“ ReLU æ•°å­¦å®šä¹‰ï¼š

f(x) = max(0, x) = {
    x,  if x > 0
    0,  if x â‰¤ 0
}

å¯¼æ•°ï¼š
f'(x) = {
    1,  if x > 0
    0,  if x â‰¤ 0
}

ğŸ” ReLU çš„ç‰¹ç‚¹ï¼š

âœ… ä¼˜ç‚¹ï¼š
â€¢ è®¡ç®—ç®€å•ï¼Œé€Ÿåº¦å¿«
â€¢ ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
â€¢ ç¨€ç–æ¿€æ´»ï¼ˆéƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºä¸º0ï¼‰
â€¢ æ— é¥±å’ŒåŒºåŸŸï¼ˆx>0æ—¶ï¼‰

âŒ ç¼ºç‚¹ï¼š
â€¢ Dead ReLU é—®é¢˜ï¼ˆè´Ÿå€¼è¢«å®Œå…¨æŠ‘åˆ¶ï¼‰
â€¢ è¾“å‡ºä¸ä»¥0ä¸ºä¸­å¿ƒ
â€¢ åœ¨x<0æ—¶æ¢¯åº¦ä¸º0ï¼Œå¯èƒ½å¯¼è‡´ç¥ç»å…ƒ"æ­»äº¡"
"""
print(relu_math)

# ============================
# 3. å¸¸ç”¨æ¿€æ´»å‡½æ•°å¯¹æ¯”
# ============================
print("\n3. å¸¸ç”¨æ¿€æ´»å‡½æ•°å¯¹æ¯”")
print("-" * 50)

def sigmoid(x):
    """Sigmoidæ¿€æ´»å‡½æ•°"""
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # é˜²æ­¢æº¢å‡º

def tanh(x):
    """Tanhæ¿€æ´»å‡½æ•°"""
    return np.tanh(x)

def leaky_relu(x, alpha=0.01):
    """Leaky ReLUæ¿€æ´»å‡½æ•°"""
    return np.where(x > 0, x, alpha * x)

def elu(x, alpha=1.0):
    """ELUæ¿€æ´»å‡½æ•°"""
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def swish(x):
    """Swishæ¿€æ´»å‡½æ•°"""
    return x * sigmoid(x)

def gelu(x):
    """GELUæ¿€æ´»å‡½æ•°ï¼ˆè¿‘ä¼¼ï¼‰"""
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

# å¯è§†åŒ–æ‰€æœ‰æ¿€æ´»å‡½æ•°
def plot_activation_functions():
    """ç»˜åˆ¶æ‰€æœ‰æ¿€æ´»å‡½æ•°çš„å›¾åƒ"""
    x = np.linspace(-5, 5, 1000)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    # å®šä¹‰æ¿€æ´»å‡½æ•°å’Œæ ‡é¢˜
    functions = [
        (relu_function, "ReLU: f(x) = max(0, x)"),
        (sigmoid, "Sigmoid: f(x) = 1/(1+e^-x)"),
        (tanh, "Tanh: f(x) = tanh(x)"),
        (leaky_relu, "Leaky ReLU: f(x) = max(0.01x, x)"),
        (elu, "ELU: f(x) = x if x>0 else Î±(e^x-1)"),
        (swish, "Swish: f(x) = xÂ·sigmoid(x)")
    ]
    
    for i, (func, title) in enumerate(functions):
        y = func(x)
        axes[i].plot(x, y, 'b-', linewidth=2)
        axes[i].set_title(title, fontsize=11)
        axes[i].grid(True, alpha=0.3)
        axes[i].axhline(y=0, color='k', linestyle='-', alpha=0.3)
        axes[i].axvline(x=0, color='k', linestyle='-', alpha=0.3)
        axes[i].set_xlabel('x')
        axes[i].set_ylabel('f(x)')
        
        # ç‰¹æ®Šæ ‡è®°
        if i == 0:  # ReLU
            axes[i].axvline(x=0, color='r', linestyle='--', alpha=0.7, label='æ‹ç‚¹')
            axes[i].legend()
    
    plt.tight_layout()
    plt.savefig('/Users/xiang/Documents/GitHub/NewStart/NewStart/activation_functions.png', 
                dpi=300, bbox_inches='tight')
    print("æ¿€æ´»å‡½æ•°å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º activation_functions.png")

try:
    plot_activation_functions()
    plt.show()
except Exception as e:
    print(f"ç»˜å›¾å‡ºé”™: {e}")

# ============================
# 4. PyTorchä¸­çš„æ¿€æ´»å‡½æ•°å®ç°
# ============================
print("\n4. PyTorchä¸­çš„æ¿€æ´»å‡½æ•°å®ç°")
print("-" * 50)

def demonstrate_pytorch_activations():
    """æ¼”ç¤ºPyTorchä¸­çš„æ¿€æ´»å‡½æ•°"""
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
    print(f"è¾“å…¥æ•°æ®: {x}")
    print()
    
    # æ–¹æ³•1: ä½¿ç”¨F.å‡½æ•°
    print("æ–¹æ³•1: ä½¿ç”¨ torch.nn.functional")
    print(f"F.relu(x):        {F.relu(x)}")
    print(f"F.sigmoid(x):     {F.sigmoid(x)}")
    print(f"F.tanh(x):        {F.tanh(x)}")
    print(f"F.leaky_relu(x):  {F.leaky_relu(x, negative_slope=0.01)}")
    print(f"F.elu(x):         {F.elu(x)}")
    print(f"F.gelu(x):        {F.gelu(x)}")
    print()
    
    # æ–¹æ³•2: ä½¿ç”¨nn.Module
    print("æ–¹æ³•2: ä½¿ç”¨ torch.nn.Module")
    activations = {
        'ReLU': nn.ReLU(),
        'Sigmoid': nn.Sigmoid(),
        'Tanh': nn.Tanh(),
        'LeakyReLU': nn.LeakyReLU(negative_slope=0.01),
        'ELU': nn.ELU(),
        'GELU': nn.GELU()
    }
    
    for name, activation in activations.items():
        result = activation(x)
        print(f"{name:10}: {result}")

demonstrate_pytorch_activations()

# ============================
# 5. ä¸åŒæ¿€æ´»å‡½æ•°åœ¨å®é™…ç½‘ç»œä¸­çš„è¡¨ç°
# ============================
print("\n5. ä¸åŒæ¿€æ´»å‡½æ•°çš„æ€§èƒ½å¯¹æ¯”")
print("-" * 50)

def test_activation_performance():
    """æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°åœ¨ç®€å•ç½‘ç»œä¸­çš„æ€§èƒ½"""
    
    # ä½¿ç”¨ä½ çš„æ•°æ®ç”Ÿæˆå‡½æ•°
    def generate_data(n_samples=1000):
        np.random.seed(42)
        class_0 = np.random.multivariate_normal([2, 2], [[1, 0.5], [0.5, 1]], n_samples//2)
        class_1 = np.random.multivariate_normal([6, 6], [[1, -0.5], [-0.5, 1]], n_samples//2)
        X = np.vstack([class_0, class_1])
        y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])
        return X, y
    
    # å®šä¹‰ä¸åŒæ¿€æ´»å‡½æ•°çš„ç½‘ç»œ
    class TestClassifier(nn.Module):
        def __init__(self, activation='relu'):
            super(TestClassifier, self).__init__()
            self.fc1 = nn.Linear(2, 64)
            self.fc2 = nn.Linear(64, 64)
            self.fc3 = nn.Linear(64, 2)
            self.dropout = nn.Dropout(0.2)
            
            # é€‰æ‹©æ¿€æ´»å‡½æ•°
            if activation == 'relu':
                self.activation = nn.ReLU()
            elif activation == 'sigmoid':
                self.activation = nn.Sigmoid()
            elif activation == 'tanh':
                self.activation = nn.Tanh()
            elif activation == 'leaky_relu':
                self.activation = nn.LeakyReLU()
            elif activation == 'elu':
                self.activation = nn.ELU()
            elif activation == 'gelu':
                self.activation = nn.GELU()
        
        def forward(self, x):
            x = self.activation(self.fc1(x))
            x = self.dropout(x)
            x = self.activation(self.fc2(x))
            x = self.dropout(x)
            x = self.fc3(x)
            return x
    
    # å‡†å¤‡æ•°æ®
    X_train, y_train = generate_data(800)
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    
    # æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°
    activations = ['relu', 'sigmoid', 'tanh', 'leaky_relu', 'elu', 'gelu']
    results = {}
    
    print("ç®€å•è®­ç»ƒæµ‹è¯• (10ä¸ªepoch):")
    print("æ¿€æ´»å‡½æ•°       æœ€ç»ˆæŸå¤±    æ”¶æ•›é€Ÿåº¦")
    print("-" * 40)
    
    for act in activations:
        model = TestClassifier(activation=act)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        # ç®€å•è®­ç»ƒ
        model.train()
        losses = []
        for epoch in range(10):
            optimizer.zero_grad()
            outputs = model(X_train_tensor)
            loss = criterion(outputs, y_train_tensor)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦ (æŸå¤±ä¸‹é™ç‡)
        convergence_rate = (losses[0] - losses[-1]) / losses[0]
        results[act] = {'final_loss': losses[-1], 'convergence': convergence_rate}
        
        print(f"{act:12}   {losses[-1]:.4f}      {convergence_rate:.3f}")
    
    return results

performance_results = test_activation_performance()

# ============================
# 6. æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—
# ============================
print("\n6. æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—")
print("-" * 50)

selection_guide = """
ğŸ¯ æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—ï¼š

ğŸ“Š **ReLU (æœ€å¸¸ç”¨)**
â€¢ é€‚ç”¨: å¤§å¤šæ•°æ·±åº¦ç½‘ç»œ
â€¢ ä¼˜åŠ¿: è®¡ç®—å¿«ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±
â€¢ åœºæ™¯: å·ç§¯ç¥ç»ç½‘ç»œã€å…¨è¿æ¥ç½‘ç»œ

ğŸ“ˆ **Leaky ReLU**
â€¢ é€‚ç”¨: ReLUå‡ºç°æ­»ç¥ç»å…ƒæ—¶
â€¢ ä¼˜åŠ¿: è§£å†³Dead ReLUé—®é¢˜
â€¢ åœºæ™¯: æ·±å±‚ç½‘ç»œï¼ŒGAN

ğŸ“‰ **ELU**
â€¢ é€‚ç”¨: éœ€è¦è´Ÿå€¼è¾“å‡ºæ—¶
â€¢ ä¼˜åŠ¿: è¾“å‡ºå‡å€¼æ¥è¿‘0
â€¢ åœºæ™¯: è‡ªç¼–ç å™¨ï¼Œæ·±å±‚ç½‘ç»œ

ğŸ”„ **Tanh**
â€¢ é€‚ç”¨: RNNï¼Œå°å‹ç½‘ç»œ
â€¢ ä¼˜åŠ¿: è¾“å‡ºèŒƒå›´(-1,1)ï¼Œ0ä¸­å¿ƒ
â€¢ åœºæ™¯: å¾ªç¯ç¥ç»ç½‘ç»œ

ğŸ“Š **Sigmoid**
â€¢ é€‚ç”¨: äºŒåˆ†ç±»è¾“å‡ºå±‚
â€¢ ä¼˜åŠ¿: è¾“å‡ºæ¦‚ç‡è§£é‡Š
â€¢ åœºæ™¯: è¾“å‡ºå±‚ï¼Œé—¨æ§æœºåˆ¶

ğŸš€ **GELU (ç°ä»£)**
â€¢ é€‚ç”¨: Transformerï¼Œç°ä»£æ¶æ„
â€¢ ä¼˜åŠ¿: å¹³æ»‘ï¼Œæ€§èƒ½ä¼˜ç§€
â€¢ åœºæ™¯: BERTã€GPTç­‰

ğŸ² **Swish**
â€¢ é€‚ç”¨: ç§»åŠ¨ç«¯æ¨¡å‹
â€¢ ä¼˜åŠ¿: è‡ªé—¨æ§ï¼Œå¹³æ»‘
â€¢ åœºæ™¯: è½»é‡çº§ç½‘ç»œ
"""
print(selection_guide)

# ============================
# 7. åœ¨ä½ çš„ä»£ç ä¸­ä¿®æ”¹æ¿€æ´»å‡½æ•°
# ============================
print("\n7. åœ¨ä½ çš„ä»£ç ä¸­ä¿®æ”¹æ¿€æ´»å‡½æ•°")
print("-" * 50)

code_modification = """
åœ¨ä½ çš„ SimpleClassifier ä¸­æ›¿æ¢æ¿€æ´»å‡½æ•°ï¼š

åŸå§‹ä»£ç :
```python
def forward(self, x):
    x = F.relu(self.fc1(x))      # â† è¿™é‡Œä½¿ç”¨ReLU
    x = self.dropout(x)
    x = F.relu(self.fc2(x))      # â† è¿™é‡Œä½¿ç”¨ReLU
    x = self.dropout(x)
    x = self.fc3(x)
    return x
```

ä¿®æ”¹ä¸ºå…¶ä»–æ¿€æ´»å‡½æ•°:
```python
# ä½¿ç”¨Leaky ReLU
x = F.leaky_relu(self.fc1(x), negative_slope=0.01)

# ä½¿ç”¨ELU
x = F.elu(self.fc1(x))

# ä½¿ç”¨GELU
x = F.gelu(self.fc1(x))

# ä½¿ç”¨Tanh
x = F.tanh(self.fc1(x))
```

æˆ–è€…åœ¨__init__ä¸­å®šä¹‰:
```python
def __init__(self, input_size, hidden_size, output_size):
    super(SimpleClassifier, self).__init__()
    self.fc1 = nn.Linear(input_size, hidden_size)
    self.fc2 = nn.Linear(hidden_size, hidden_size)
    self.fc3 = nn.Linear(hidden_size, output_size)
    self.dropout = nn.Dropout(0.2)
    self.activation = nn.GELU()  # â† å®šä¹‰æ¿€æ´»å‡½æ•°

def forward(self, x):
    x = self.activation(self.fc1(x))  # â† ä½¿ç”¨å®šä¹‰çš„æ¿€æ´»å‡½æ•°
    x = self.dropout(x)
    x = self.activation(self.fc2(x))
    x = self.dropout(x)
    x = self.fc3(x)
    return x
```
"""
print(code_modification)

# ============================
# 8. æ¢¯åº¦æµåŠ¨å¯¹æ¯”
# ============================
print("\n8. æ¢¯åº¦æµåŠ¨å¯¹æ¯”")
print("-" * 50)

def visualize_gradients():
    """å¯è§†åŒ–ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦"""
    x = np.linspace(-3, 3, 1000)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    axes = axes.flatten()
    
    functions_and_derivatives = [
        (relu_function, relu_derivative, "ReLU"),
        (sigmoid, lambda x: sigmoid(x) * (1 - sigmoid(x)), "Sigmoid"),
        (tanh, lambda x: 1 - tanh(x)**2, "Tanh"),
        (leaky_relu, lambda x: np.where(x > 0, 1, 0.01), "Leaky ReLU"),
        (elu, lambda x: np.where(x > 0, 1, elu(x) + 1), "ELU"),
        (swish, lambda x: sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x)), "Swish")
    ]
    
    for i, (func, derivative, name) in enumerate(functions_and_derivatives):
        y = func(x)
        dy = derivative(x)
        
        # å‡½æ•°å›¾åƒ
        ax = axes[i]
        ax.plot(x, y, 'b-', linewidth=2, label='å‡½æ•°')
        ax.plot(x, dy, 'r--', linewidth=2, label='å¯¼æ•°')
        ax.set_title(f'{name}', fontsize=12)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8)
        ax.set_xlim(-3, 3)
        
        # æ ‡è®°æ¢¯åº¦æ¶ˆå¤±åŒºåŸŸ
        if name in ['Sigmoid', 'Tanh']:
            ax.axvspan(-3, -2, alpha=0.2, color='red', label='æ¢¯åº¦å°')
            ax.axvspan(2, 3, alpha=0.2, color='red')
    
    plt.tight_layout()
    plt.savefig('/Users/xiang/Documents/GitHub/NewStart/NewStart/activation_gradients.png', 
                dpi=300, bbox_inches='tight')
    print("æ¿€æ´»å‡½æ•°æ¢¯åº¦å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º activation_gradients.png")

try:
    visualize_gradients()
    plt.show()
except Exception as e:
    print(f"æ¢¯åº¦å¯è§†åŒ–å‡ºé”™: {e}")

# ============================
# 9. æ€»ç»“
# ============================
print("\n" + "="*70)
print("æ€»ç»“")
print("="*70)

summary = """
ğŸ“ æ¿€æ´»å‡½æ•°æ€»ç»“ï¼š

ğŸ”¥ **ReLUçš„é‡è¦æ€§**ï¼š
â€¢ åœ¨ä½ çš„ä»£ç ä¸­ä½¿ç”¨F.relu()æ˜¯æ˜æ™ºé€‰æ‹©
â€¢ è®¡ç®—æ•ˆç‡é«˜ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
â€¢ æœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
â€¢ é€‚åˆå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡

ğŸ“ˆ **é€‰æ‹©å»ºè®®**ï¼š
1. **é»˜è®¤é€‰æ‹©**: ReLU (99%æƒ…å†µä¸‹éƒ½æ²¡é—®é¢˜)
2. **é‡åˆ°æ­»ç¥ç»å…ƒ**: è¯•è¯•Leaky ReLUæˆ–ELU
3. **ç°ä»£æ¶æ„**: è€ƒè™‘GELU (Transformerç­‰)
4. **è¾“å‡ºå±‚**: 
   - äºŒåˆ†ç±»: Sigmoid
   - å¤šåˆ†ç±»: ä¸ç”¨æ¿€æ´»å‡½æ•° (é…åˆCrossEntropyLoss)
   - å›å½’: ä¸ç”¨æ¿€æ´»å‡½æ•°æˆ–æ ¹æ®è¾“å‡ºèŒƒå›´é€‰æ‹©

ğŸš€ **ä¼˜åŒ–å»ºè®®**ï¼š
â€¢ ä»ReLUå¼€å§‹ï¼Œæœ‰é—®é¢˜å†æ¢
â€¢ ä¸åŒå±‚å¯ä»¥ä½¿ç”¨ä¸åŒæ¿€æ´»å‡½æ•°
â€¢ æ³¨æ„æ¿€æ´»å‡½æ•°å¯¹å­¦ä¹ ç‡çš„å½±å“
â€¢ ç»“åˆBatchNormä½¿ç”¨æ•ˆæœæ›´å¥½

ğŸ’¡ **åœ¨ä½ çš„PyTorchä»£ç ä¸­**ï¼š
å½“å‰ä½¿ç”¨ReLUæ˜¯æœ€ä½³å®è·µï¼Œå¦‚æœæƒ³æå‡æ€§èƒ½ï¼Œå¯ä»¥å°è¯•ï¼š
1. å°†ReLUæ”¹ä¸ºGELU
2. æ·»åŠ BatchNormå±‚
3. å°è¯•ä¸åŒçš„å­¦ä¹ ç‡
"""
print(summary)

if __name__ == "__main__":
    print("\nğŸ‰ æ¿€æ´»å‡½æ•°è¯¦è§£å®Œæˆï¼")