import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

print("=" * 70)
print("loss.backward() å¦‚ä½•æ‰¾åˆ°å¹¶æ›´æ–°æ¨¡å‹å‚æ•°çš„æœºåˆ¶è¯¦è§£")
print("=" * 70)

# ============================
# 1. PyTorchè®¡ç®—å›¾æœºåˆ¶
# ============================
print("\n1. PyTorchè®¡ç®—å›¾æœºåˆ¶")
print("-" * 50)

computational_graph_explanation = """
ğŸ”— è®¡ç®—å›¾ (Computational Graph) çš„å·¥ä½œåŸç†ï¼š

ğŸ“Š **è‡ªåŠ¨æ„å»ºè®¡ç®—å›¾**ï¼š
â€¢ PyTorchåœ¨å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ„å»ºè®¡ç®—å›¾
â€¢ æ¯ä¸ªtensoréƒ½è®°å½•äº†å®ƒæ˜¯å¦‚ä½•ä»å…¶ä»–tensorè®¡ç®—å¾—æ¥çš„
â€¢ æ¯ä¸ªæ“ä½œéƒ½ä¼šåœ¨å›¾ä¸­åˆ›å»ºä¸€ä¸ªèŠ‚ç‚¹

ğŸ¯ **æ¢¯åº¦ä¼ æ’­è·¯å¾„**ï¼š
â€¢ loss.backward() æ²¿ç€è®¡ç®—å›¾åå‘éå†
â€¢ ä»losså¼€å§‹ï¼Œé€å±‚å‘å‰æ‰¾åˆ°æ‰€æœ‰éœ€è¦æ¢¯åº¦çš„å‚æ•°
â€¢ è‡ªåŠ¨åº”ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦

ğŸ” **å‚æ•°å‘ç°æœºåˆ¶**ï¼š
â€¢ æ‰€æœ‰requires_grad=Trueçš„tensoréƒ½ä¼šè¢«è·Ÿè¸ª
â€¢ æ¨¡å‹å‚æ•°é»˜è®¤requires_grad=True
â€¢ backward()ä¼šæ‰¾åˆ°æ‰€æœ‰è¿™äº›å‚æ•°å¹¶è®¡ç®—æ¢¯åº¦
"""
print(computational_graph_explanation)

# ============================
# 2. æ¼”ç¤ºè®¡ç®—å›¾çš„æ„å»ºè¿‡ç¨‹
# ============================
print("\n2. æ¼”ç¤ºè®¡ç®—å›¾çš„æ„å»ºè¿‡ç¨‹")
print("-" * 50)

def demonstrate_computation_graph():
    """æ¼”ç¤ºè®¡ç®—å›¾çš„æ„å»ºå’Œæ¢¯åº¦è®¡ç®—"""
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    class TinyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(2, 1, bias=False)
            # æ‰‹åŠ¨è®¾ç½®æƒé‡ä¾¿äºè§‚å¯Ÿ
            self.linear.weight.data = torch.tensor([[1.0, 2.0]])
        
        def forward(self, x):
            return self.linear(x)
    
    model = TinyModel()
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    x = torch.tensor([[1.0, 1.0]], requires_grad=False)
    target = torch.tensor([[5.0]])
    
    print("ğŸ” è¿½è¸ªè®¡ç®—å›¾æ„å»ºè¿‡ç¨‹ï¼š")
    print(f"æ¨¡å‹æƒé‡: {model.linear.weight}")
    print(f"è¾“å…¥æ•°æ®: {x}")
    print(f"ç›®æ ‡å€¼: {target}")
    print()
    
    # å‰å‘ä¼ æ’­
    print("1ï¸âƒ£ å‰å‘ä¼ æ’­ - æ„å»ºè®¡ç®—å›¾ï¼š")
    output = model(x)  # è¿™é‡Œæ„å»ºäº†è®¡ç®—å›¾
    print(f"   æ¨¡å‹è¾“å‡º: {output}")
    print(f"   è¾“å‡ºçš„grad_fn: {output.grad_fn}")  # æ˜¾ç¤ºè®¡ç®—å›¾èŠ‚ç‚¹
    print()
    
    # è®¡ç®—æŸå¤±
    loss = F.mse_loss(output, target)
    print(f"2ï¸âƒ£ è®¡ç®—æŸå¤±:")
    print(f"   æŸå¤±å€¼: {loss}")
    print(f"   æŸå¤±çš„grad_fn: {loss.grad_fn}")  # æŸå¤±å‡½æ•°çš„è®¡ç®—å›¾èŠ‚ç‚¹
    print()
    
    # æŸ¥çœ‹å‚æ•°çš„æ¢¯åº¦çŠ¶æ€
    print("3ï¸âƒ£ åå‘ä¼ æ’­å‰çš„æ¢¯åº¦çŠ¶æ€:")
    print(f"   æƒé‡çš„requires_grad: {model.linear.weight.requires_grad}")
    print(f"   æƒé‡çš„grad: {model.linear.weight.grad}")
    print()
    
    # åå‘ä¼ æ’­
    print("4ï¸âƒ£ æ‰§è¡Œ loss.backward():")
    loss.backward()  # æ²¿è®¡ç®—å›¾åå‘ä¼ æ’­
    print(f"   åå‘ä¼ æ’­å®Œæˆï¼")
    print(f"   æƒé‡çš„æ¢¯åº¦: {model.linear.weight.grad}")
    print()
    
    return model

model_demo = demonstrate_computation_graph()

# ============================
# 3. è¯¦ç»†åˆ†ægrad_fné“¾æ¡
# ============================
print("\n3. è¯¦ç»†åˆ†ægrad_fné“¾æ¡")
print("-" * 50)

def analyze_grad_fn_chain():
    """åˆ†æè®¡ç®—å›¾ä¸­çš„grad_fné“¾æ¡"""
    
    class SimpleNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(2, 3)
            self.fc2 = nn.Linear(3, 1)
        
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    model = SimpleNet()
    x = torch.randn(1, 2)
    
    # å‰å‘ä¼ æ’­å¹¶è¿½è¸ªæ¯ä¸€æ­¥
    print("ğŸ”— è¿½è¸ªè®¡ç®—å›¾é“¾æ¡ï¼š")
    
    # ç¬¬ä¸€å±‚
    z1 = model.fc1(x)
    print(f"1. fc1è¾“å‡ºçš„grad_fn: {z1.grad_fn}")
    
    # ReLUæ¿€æ´»
    a1 = F.relu(z1)
    print(f"2. ReLUè¾“å‡ºçš„grad_fn: {a1.grad_fn}")
    
    # ç¬¬äºŒå±‚
    z2 = model.fc2(a1)
    print(f"3. fc2è¾“å‡ºçš„grad_fn: {z2.grad_fn}")
    
    # æŸå¤±
    target = torch.randn(1, 1)
    loss = F.mse_loss(z2, target)
    print(f"4. æŸå¤±çš„grad_fn: {loss.grad_fn}")
    print()
    
    # æ˜¾ç¤ºå‚æ•°ä¸è®¡ç®—å›¾çš„è¿æ¥
    print("ğŸ“ å‚æ•°åœ¨è®¡ç®—å›¾ä¸­çš„ä½ç½®ï¼š")
    for name, param in model.named_parameters():
        print(f"   {name}: requires_grad={param.requires_grad}")
    
    return model, loss

model_chain, loss_chain = analyze_grad_fn_chain()

# ============================
# 4. æ¼”ç¤ºoptimizerå¦‚ä½•æ‰¾åˆ°å‚æ•°
# ============================
print("\n4. æ¼”ç¤ºoptimizerå¦‚ä½•æ‰¾åˆ°å‚æ•°")
print("-" * 50)

def demonstrate_optimizer_parameter_access():
    """æ¼”ç¤ºoptimizerå¦‚ä½•è®¿é—®å’Œæ›´æ–°å‚æ•°"""
    
    # åˆ›å»ºæ¨¡å‹
    model = nn.Linear(2, 1)
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    
    print("ğŸ›ï¸ Optimizerå¯¹å‚æ•°çš„è®¿é—®ï¼š")
    
    # æŸ¥çœ‹optimizerä¸­å­˜å‚¨çš„å‚æ•°
    print("1ï¸âƒ£ Optimizerä¸­çš„å‚æ•°ç»„ï¼š")
    for i, param_group in enumerate(optimizer.param_groups):
        print(f"   å‚æ•°ç»„ {i}: å­¦ä¹ ç‡ = {param_group['lr']}")
        print(f"   å‚æ•°æ•°é‡: {len(param_group['params'])}")
        for j, param in enumerate(param_group['params']):
            print(f"     å‚æ•° {j}: å½¢çŠ¶ = {param.shape}, requires_grad = {param.requires_grad}")
    print()
    
    # åˆ›å»ºæ•°æ®è¿›è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤
    x = torch.randn(5, 2)
    y = torch.randn(5, 1)
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = F.mse_loss(output, y)
    
    print("2ï¸âƒ£ è®­ç»ƒæ­¥éª¤æ¼”ç¤ºï¼š")
    print(f"   æŸå¤±å€¼: {loss.item():.4f}")
    
    # ä¿å­˜æ›´æ–°å‰çš„å‚æ•°
    old_weight = model.weight.clone()
    old_bias = model.bias.clone()
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    
    print(f"   åå‘ä¼ æ’­åæƒé‡æ¢¯åº¦: {model.weight.grad}")
    print(f"   åå‘ä¼ æ’­ååç½®æ¢¯åº¦: {model.bias.grad}")
    
    # å‚æ•°æ›´æ–°
    optimizer.step()
    
    # æ˜¾ç¤ºå‚æ•°å˜åŒ–
    weight_change = (model.weight - old_weight).norm().item()
    bias_change = (model.bias - old_bias).norm().item()
    
    print(f"   æƒé‡å˜åŒ–å¤§å°: {weight_change:.6f}")
    print(f"   åç½®å˜åŒ–å¤§å°: {bias_change:.6f}")
    print()

demonstrate_optimizer_parameter_access()

# ============================
# 5. æ‰‹åŠ¨å®ç°å‚æ•°æ›´æ–°æ¥ç†è§£æœºåˆ¶
# ============================
print("\n5. æ‰‹åŠ¨å®ç°å‚æ•°æ›´æ–°æ¥ç†è§£æœºåˆ¶")
print("-" * 50)

def manual_parameter_update():
    """æ‰‹åŠ¨å®ç°å‚æ•°æ›´æ–°ï¼Œå±•ç¤ºoptimizer.step()çš„å†…éƒ¨å·¥ä½œ"""
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Linear(2, 1)
    
    # åˆ›å»ºæ•°æ®
    x = torch.randn(3, 2)
    y = torch.randn(3, 1)
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = F.mse_loss(output, y)
    
    print("ğŸ”§ æ‰‹åŠ¨å‚æ•°æ›´æ–°æ¼”ç¤ºï¼š")
    print(f"åˆå§‹æŸå¤±: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    print("\næ–¹æ³•1: ä½¿ç”¨optimizer.step()")
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    optimizer.step()
    
    # é‡æ–°è®¡ç®—æŸå¤±éªŒè¯
    output_after = model(x)
    loss_after = F.mse_loss(output_after, y)
    print(f"ä½¿ç”¨optimizeråçš„æŸå¤±: {loss_after.item():.4f}")
    
    # é‡ç½®æ¨¡å‹å¹¶æ‰‹åŠ¨æ›´æ–°
    model = nn.Linear(2, 1)
    output = model(x)
    loss = F.mse_loss(output, y)
    loss.backward()
    
    print("\næ–¹æ³•2: æ‰‹åŠ¨æ›´æ–°å‚æ•°")
    lr = 0.1
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for param in model.parameters():
            if param.grad is not None:
                param -= lr * param.grad  # æ‰‹åŠ¨åº”ç”¨æ¢¯åº¦æ›´æ–°
    
    # éªŒè¯æ‰‹åŠ¨æ›´æ–°çš„æ•ˆæœ
    output_manual = model(x)
    loss_manual = F.mse_loss(output_manual, y)
    print(f"æ‰‹åŠ¨æ›´æ–°åçš„æŸå¤±: {loss_manual.item():.4f}")
    
    print("\nâœ… ä¸¤ç§æ–¹æ³•å¾—åˆ°ç›¸åŒç»“æœï¼Œè¯æ˜optimizer.step()å°±æ˜¯åœ¨è‡ªåŠ¨æ‰§è¡Œæ‰‹åŠ¨æ›´æ–°çš„è¿‡ç¨‹")

manual_parameter_update()

# ============================
# 6. å®Œæ•´çš„å‚æ•°æ›´æ–°æµç¨‹å›¾
# ============================
print("\n6. å®Œæ•´çš„å‚æ•°æ›´æ–°æµç¨‹")
print("-" * 50)

complete_flow = """
ğŸ”„ å®Œæ•´çš„å‚æ•°æ›´æ–°æµç¨‹ï¼š

ğŸ“‹ **è®­ç»ƒå¾ªç¯ä¸­çš„æ­¥éª¤**ï¼š

1ï¸âƒ£ **optimizer.zero_grad()**
   â€¢ æ¸…é™¤æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
   â€¢ é˜²æ­¢æ¢¯åº¦ç´¯ç§¯
   
2ï¸âƒ£ **å‰å‘ä¼ æ’­: output = model(input)**
   â€¢ æ„å»ºè®¡ç®—å›¾
   â€¢ æ¯ä¸ªæ“ä½œéƒ½è®°å½•åœ¨å›¾ä¸­
   â€¢ è¾“å‡ºtensoråŒ…å«grad_fnä¿¡æ¯

3ï¸âƒ£ **è®¡ç®—æŸå¤±: loss = criterion(output, target)**
   â€¢ æŸå¤±å‡½æ•°ä¹Ÿæ˜¯è®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†
   â€¢ loss tensoråŒ…å«å®Œæ•´çš„è®¡ç®—å†å²

4ï¸âƒ£ **åå‘ä¼ æ’­: loss.backward()**
   â€¢ ä»losså¼€å§‹ï¼Œæ²¿è®¡ç®—å›¾åå‘éå†
   â€¢ è‡ªåŠ¨æ‰¾åˆ°æ‰€æœ‰requires_grad=Trueçš„å‚æ•°
   â€¢ ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
   â€¢ å°†æ¢¯åº¦å­˜å‚¨åœ¨param.gradä¸­

5ï¸âƒ£ **å‚æ•°æ›´æ–°: optimizer.step()**
   â€¢ éå†optimizerä¸­æ³¨å†Œçš„æ‰€æœ‰å‚æ•°
   â€¢ å¯¹æ¯ä¸ªå‚æ•°åº”ç”¨æ›´æ–°è§„åˆ™
   â€¢ SGD: param = param - lr * param.grad
   â€¢ Adam: æ›´å¤æ‚çš„è‡ªé€‚åº”æ›´æ–°

ğŸ”— **å…³é”®è¿æ¥**ï¼š
â€¢ model.parameters() â†’ optimizeræ³¨å†Œå‚æ•°
â€¢ å‰å‘ä¼ æ’­ â†’ è®¡ç®—å›¾æ„å»º
â€¢ backward() â†’ æ¢¯åº¦è®¡ç®—
â€¢ optimizer.step() â†’ å‚æ•°æ›´æ–°

ğŸ’¡ **æ ¸å¿ƒç†è§£**ï¼š
loss.backward()ä¸ç›´æ¥æ›´æ–°å‚æ•°ï¼Œå®ƒåªæ˜¯è®¡ç®—æ¢¯åº¦ï¼
çœŸæ­£çš„å‚æ•°æ›´æ–°ç”±optimizer.step()å®Œæˆï¼
"""
print(complete_flow)

# ============================
# 7. åœ¨ä½ çš„ä»£ç ä¸­çš„ä½“ç°
# ============================
print("\n7. åœ¨ä½ çš„ä»£ç ä¸­çš„ä½“ç°")
print("-" * 50)

your_code_analysis = """
ğŸ“ åœ¨ä½ çš„PyTorchä»£ç ä¸­çš„ä½“ç°ï¼š

```python
# åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
model = SimpleClassifier(input_size=2, hidden_size=64, output_size=2)
optimizer = optim.Adam(model.parameters(), lr=0.001)  # â† optimizeræ³¨å†Œäº†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°

# è®­ç»ƒå¾ªç¯
for batch_idx, (data, target) in enumerate(train_loader):
    
    optimizer.zero_grad()        # â† æ¸…é™¤æ¢¯åº¦
    
    outputs = model(data)        # â† å‰å‘ä¼ æ’­ï¼Œæ„å»ºè®¡ç®—å›¾
    loss = criterion(outputs, target)  # â† è®¡ç®—æŸå¤±ï¼Œæ‰©å±•è®¡ç®—å›¾
    
    loss.backward()              # â† æ²¿è®¡ç®—å›¾åå‘ä¼ æ’­ï¼Œè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
    
    optimizer.step()             # â† ä½¿ç”¨è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°å‚æ•°
```

ğŸ” **è¯¦ç»†è§£æ**ï¼š

1. **optimizer.zero_grad()**:
   â€¢ éå†optimizerä¸­çš„æ‰€æœ‰å‚æ•°
   â€¢ å°†æ¯ä¸ªparam.gradè®¾ä¸ºNoneæˆ–é›¶å¼ é‡

2. **outputs = model(data)**:
   â€¢ æ•°æ®flow: data â†’ fc1 â†’ relu â†’ dropout â†’ fc2 â†’ relu â†’ dropout â†’ fc3 â†’ outputs
   â€¢ æ¯ä¸€æ­¥éƒ½åœ¨è®¡ç®—å›¾ä¸­è®°å½•
   â€¢ outputs.grad_fnæŒ‡å‘æ•´ä¸ªè®¡ç®—é“¾

3. **loss = criterion(outputs, target)**:
   â€¢ CrossEntropyLossè®¡ç®—ä¹ŸåŠ å…¥è®¡ç®—å›¾
   â€¢ loss.grad_fnè¿æ¥åˆ°æ•´ä¸ªç½‘ç»œ

4. **loss.backward()**:
   â€¢ ä»losså¼€å§‹åå‘ä¼ æ’­
   â€¢ è‡ªåŠ¨æ‰¾åˆ°fc1.weight, fc1.bias, fc2.weight, fc2.bias, fc3.weight, fc3.bias
   â€¢ è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦å¹¶å­˜å‚¨åœ¨param.gradä¸­

5. **optimizer.step()**:
   â€¢ Adamä¼˜åŒ–å™¨éå†æ‰€æœ‰æ³¨å†Œçš„å‚æ•°
   â€¢ å¯¹æ¯ä¸ªå‚æ•°åº”ç”¨Adamæ›´æ–°è§„åˆ™
   â€¢ å®é™…ä¿®æ”¹å‚æ•°å€¼

ğŸ¯ **å…³é”®ç†è§£**ï¼š
â€¢ loss.backward()æ˜¯"è®¡ç®—"æ¢¯åº¦
â€¢ optimizer.step()æ˜¯"åº”ç”¨"æ¢¯åº¦
â€¢ è®¡ç®—å›¾æ˜¯è¿æ¥ä¸¤è€…çš„æ¡¥æ¢
"""
print(your_code_analysis)

# ============================
# 8. éªŒè¯å‚æ•°è¿æ¥å…³ç³»
# ============================
print("\n8. éªŒè¯å‚æ•°è¿æ¥å…³ç³»")
print("-" * 50)

def verify_parameter_connection():
    """éªŒè¯æ¨¡å‹å‚æ•°ä¸optimizerçš„è¿æ¥å…³ç³»"""
    
    # åˆ›å»ºä¸ä½ ä»£ç ç›¸åŒçš„æ¨¡å‹
    class SimpleClassifier(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(SimpleClassifier, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, output_size)
            self.dropout = nn.Dropout(0.2)
            
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.dropout(x)
            x = F.relu(self.fc2(x))
            x = self.dropout(x)
            x = self.fc3(x)
            return x
    
    model = SimpleClassifier(2, 4, 2)  # ç®€åŒ–ç‰ˆæœ¬
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    print("ğŸ”— éªŒè¯å‚æ•°è¿æ¥å…³ç³»ï¼š")
    
    # 1. æ˜¾ç¤ºæ¨¡å‹å‚æ•°
    print("1ï¸âƒ£ æ¨¡å‹ä¸­çš„å‚æ•°ï¼š")
    model_params = list(model.parameters())
    for i, param in enumerate(model_params):
        print(f"   å‚æ•° {i}: å½¢çŠ¶ {param.shape}, id = {id(param)}")
    
    print()
    
    # 2. æ˜¾ç¤ºoptimizerä¸­çš„å‚æ•°
    print("2ï¸âƒ£ Optimizerä¸­çš„å‚æ•°ï¼š")
    optimizer_params = optimizer.param_groups[0]['params']
    for i, param in enumerate(optimizer_params):
        print(f"   å‚æ•° {i}: å½¢çŠ¶ {param.shape}, id = {id(param)}")
    
    print()
    
    # 3. éªŒè¯æ˜¯å¦æ˜¯åŒä¸€ä¸ªå¯¹è±¡
    print("3ï¸âƒ£ éªŒè¯å‚æ•°èº«ä»½ä¸€è‡´æ€§ï¼š")
    for i, (model_param, opt_param) in enumerate(zip(model_params, optimizer_params)):
        is_same = model_param is opt_param
        print(f"   å‚æ•° {i}: åŒä¸€å¯¹è±¡ = {is_same}")
    
    print()
    print("âœ… ç»“è®º: optimizerå’Œmodelå…±äº«ç›¸åŒçš„å‚æ•°å¯¹è±¡å¼•ç”¨")
    print("   è¿™å°±æ˜¯ä¸ºä»€ä¹ˆloss.backward()è®¡ç®—çš„æ¢¯åº¦èƒ½è¢«optimizer.step()ä½¿ç”¨")

verify_parameter_connection()

# ============================
# 9. æ€»ç»“
# ============================
print("\n" + "="*70)
print("æ€»ç»“")
print("="*70)

summary = """
ğŸ“ loss.backward() å¦‚ä½•æ›´æ–°æ¨¡å‹å‚æ•°çš„å®Œæ•´æœºåˆ¶ï¼š

ğŸ”— **æ ¸å¿ƒæœºåˆ¶ - è®¡ç®—å›¾**ï¼š
   â€¢ PyTorchè‡ªåŠ¨æ„å»ºè®¡ç®—å›¾ï¼Œè®°å½•æ‰€æœ‰æ“ä½œ
   â€¢ æ¯ä¸ªtensoréƒ½çŸ¥é“å®ƒæ˜¯å¦‚ä½•è®¡ç®—å‡ºæ¥çš„
   â€¢ loss.backward()æ²¿ç€è¿™ä¸ªå›¾æ‰¾åˆ°æ‰€æœ‰éœ€è¦æ¢¯åº¦çš„å‚æ•°

ğŸ¯ **ä¸‰ä¸ªå…³é”®è¿æ¥**ï¼š
   1. æ¨¡å‹å‚æ•° â†â†’ Optimizerå‚æ•°ï¼ˆå…±äº«å¯¹è±¡å¼•ç”¨ï¼‰
   2. å‰å‘ä¼ æ’­ â†â†’ è®¡ç®—å›¾æ„å»ºï¼ˆè‡ªåŠ¨è®°å½•ï¼‰
   3. åå‘ä¼ æ’­ â†â†’ æ¢¯åº¦è®¡ç®—ï¼ˆè‡ªåŠ¨å¾®åˆ†ï¼‰

âš¡ **æ›´æ–°æµç¨‹**ï¼š
   1. optimizer.zero_grad()  # æ¸…é™¤æ—§æ¢¯åº¦
   2. output = model(input)  # æ„å»ºè®¡ç®—å›¾
   3. loss = criterion(...)  # æ‰©å±•è®¡ç®—å›¾
   4. loss.backward()        # è®¡ç®—æ¢¯åº¦ï¼ˆä¸æ›´æ–°å‚æ•°ï¼ï¼‰
   5. optimizer.step()       # åº”ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°

ğŸ’¡ **å…³é”®ç†è§£**ï¼š
   â€¢ loss.backward() åªè®¡ç®—æ¢¯åº¦ï¼Œä¸æ›´æ–°å‚æ•°
   â€¢ optimizer.step() æ‰çœŸæ­£æ›´æ–°å‚æ•°
   â€¢ ä¸¤è€…é€šè¿‡å…±äº«çš„å‚æ•°å¯¹è±¡å¼•ç”¨è¿æ¥
   â€¢ è®¡ç®—å›¾æ˜¯è‡ªåŠ¨å¾®åˆ†çš„åŸºç¡€

ğŸš€ **åœ¨ä½ çš„ä»£ç ä¸­**ï¼š
   æ‰€æœ‰è¿™äº›éƒ½è‡ªåŠ¨å‘ç”Ÿï¼Œä½ åªéœ€è¦è°ƒç”¨ï¼š
   â€¢ loss.backward()  # PyTorchæ‰¾åˆ°æ‰€æœ‰å‚æ•°å¹¶è®¡ç®—æ¢¯åº¦
   â€¢ optimizer.step() # Adamä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°

è¿™å°±æ˜¯PyTorch"è‡ªåŠ¨"çš„é­…åŠ›æ‰€åœ¨ï¼ğŸ‰
"""
print(summary)

if __name__ == "__main__":
    print("\nğŸ‰ å‚æ•°æ›´æ–°æœºåˆ¶è§£æå®Œæˆï¼")